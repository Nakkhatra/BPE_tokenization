{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['God',\n",
       " 'of',\n",
       " 'War',\n",
       " 'is',\n",
       " 'an',\n",
       " 'action-adventure',\n",
       " 'game',\n",
       " 'franchise',\n",
       " 'created',\n",
       " 'by',\n",
       " 'David',\n",
       " 'Jaffe',\n",
       " 'at',\n",
       " \"Sony's\",\n",
       " 'Santa',\n",
       " 'Monica',\n",
       " 'Studio.',\n",
       " 'It',\n",
       " 'began',\n",
       " 'in',\n",
       " '2005',\n",
       " 'on',\n",
       " 'the',\n",
       " 'PlayStation',\n",
       " '2',\n",
       " '(PS2)',\n",
       " 'video',\n",
       " 'game',\n",
       " 'console',\n",
       " 'and',\n",
       " 'has',\n",
       " 'become',\n",
       " 'a',\n",
       " 'flagship',\n",
       " 'title',\n",
       " 'for',\n",
       " 'the',\n",
       " 'PlayStation',\n",
       " 'brand',\n",
       " 'consisting',\n",
       " 'of',\n",
       " 'eight',\n",
       " 'games',\n",
       " 'across',\n",
       " 'multiple',\n",
       " 'platforms',\n",
       " 'with',\n",
       " 'a',\n",
       " 'ninth',\n",
       " 'currently',\n",
       " 'in',\n",
       " 'development.',\n",
       " 'Based',\n",
       " 'in',\n",
       " 'ancient',\n",
       " 'mythology',\n",
       " 'the',\n",
       " 'story',\n",
       " 'follows',\n",
       " 'the',\n",
       " 'titular',\n",
       " 'protagonist',\n",
       " 'Kratos',\n",
       " 'a',\n",
       " 'Spartan',\n",
       " 'warrior',\n",
       " 'and',\n",
       " 'later',\n",
       " 'the',\n",
       " 'God',\n",
       " 'of',\n",
       " 'War',\n",
       " 'who',\n",
       " 'was',\n",
       " 'tricked',\n",
       " 'into',\n",
       " 'killing',\n",
       " 'his',\n",
       " 'family',\n",
       " 'by',\n",
       " 'his',\n",
       " 'former',\n",
       " 'master',\n",
       " 'the',\n",
       " 'original',\n",
       " 'Greek',\n",
       " 'god',\n",
       " 'of',\n",
       " 'war',\n",
       " 'Ares.',\n",
       " 'This',\n",
       " 'sets',\n",
       " 'off',\n",
       " 'a',\n",
       " 'series',\n",
       " 'of',\n",
       " 'events',\n",
       " 'that',\n",
       " 'leads',\n",
       " 'to',\n",
       " 'wars',\n",
       " 'with',\n",
       " 'the',\n",
       " 'mythological',\n",
       " 'pantheons.',\n",
       " 'The',\n",
       " 'Greek',\n",
       " 'mythology',\n",
       " 'era',\n",
       " 'of',\n",
       " 'the',\n",
       " 'series',\n",
       " 'sees',\n",
       " 'Kratos',\n",
       " 'follow',\n",
       " 'a',\n",
       " 'path',\n",
       " 'of',\n",
       " 'vengeance',\n",
       " 'due',\n",
       " 'to',\n",
       " 'the',\n",
       " 'machinations',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Olympian',\n",
       " 'gods',\n",
       " 'while',\n",
       " 'the',\n",
       " 'Norse',\n",
       " 'mythology',\n",
       " 'era',\n",
       " 'which',\n",
       " 'introduces',\n",
       " 'his',\n",
       " 'son',\n",
       " 'Atreus',\n",
       " 'as',\n",
       " 'a',\n",
       " 'secondary',\n",
       " 'protagonist',\n",
       " 'shows',\n",
       " 'an',\n",
       " 'older',\n",
       " 'Kratos',\n",
       " 'on',\n",
       " 'a',\n",
       " 'path',\n",
       " 'of',\n",
       " 'redemption',\n",
       " 'which',\n",
       " 'inadvertently',\n",
       " 'brings',\n",
       " 'the',\n",
       " 'two',\n",
       " 'into',\n",
       " 'conflict',\n",
       " 'with',\n",
       " 'the',\n",
       " 'Norse',\n",
       " 'gods.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing library\n",
    "import pandas as pd\n",
    "\n",
    "#reading .txt file\n",
    "text = pd.read_csv(\"demo.txt\",header=None)\n",
    "# print((text.values))\n",
    "\n",
    "#converting a dataframe into a single list \n",
    "corpus=[]\n",
    "words = \"\"\n",
    "for row in text.values:\n",
    "    for line in row:\n",
    "        words += line\n",
    "tokens = words.split(\" \")\n",
    "for token in tokens:\n",
    "    corpus.append(token)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['G o d </w>',\n",
       " 'o f </w>',\n",
       " 'W a r </w>',\n",
       " 'i s </w>',\n",
       " 'a n </w>',\n",
       " 'a c t i o n - a d v e n t u r e </w>',\n",
       " 'g a m e </w>',\n",
       " 'f r a n c h i s e </w>',\n",
       " 'c r e a t e d </w>',\n",
       " 'b y </w>',\n",
       " 'D a v i d </w>',\n",
       " 'J a f f e </w>',\n",
       " 'a t </w>',\n",
       " \"S o n y ' s </w>\",\n",
       " 'S a n t a </w>',\n",
       " 'M o n i c a </w>',\n",
       " 'S t u d i o . </w>',\n",
       " 'I t </w>',\n",
       " 'b e g a n </w>',\n",
       " 'i n </w>',\n",
       " '2 0 0 5 </w>',\n",
       " 'o n </w>',\n",
       " 't h e </w>',\n",
       " 'P l a y S t a t i o n </w>',\n",
       " '2 </w>',\n",
       " '( P S 2 ) </w>',\n",
       " 'v i d e o </w>',\n",
       " 'g a m e </w>',\n",
       " 'c o n s o l e </w>',\n",
       " 'a n d </w>',\n",
       " 'h a s </w>',\n",
       " 'b e c o m e </w>',\n",
       " 'a </w>',\n",
       " 'f l a g s h i p </w>',\n",
       " 't i t l e </w>',\n",
       " 'f o r </w>',\n",
       " 't h e </w>',\n",
       " 'P l a y S t a t i o n </w>',\n",
       " 'b r a n d </w>',\n",
       " 'c o n s i s t i n g </w>',\n",
       " 'o f </w>',\n",
       " 'e i g h t </w>',\n",
       " 'g a m e s </w>',\n",
       " 'a c r o s s </w>',\n",
       " 'm u l t i p l e </w>',\n",
       " 'p l a t f o r m s </w>',\n",
       " 'w i t h </w>',\n",
       " 'a </w>',\n",
       " 'n i n t h </w>',\n",
       " 'c u r r e n t l y </w>',\n",
       " 'i n </w>',\n",
       " 'd e v e l o p m e n t . </w>',\n",
       " 'B a s e d </w>',\n",
       " 'i n </w>',\n",
       " 'a n c i e n t </w>',\n",
       " 'm y t h o l o g y </w>',\n",
       " 't h e </w>',\n",
       " 's t o r y </w>',\n",
       " 'f o l l o w s </w>',\n",
       " 't h e </w>',\n",
       " 't i t u l a r </w>',\n",
       " 'p r o t a g o n i s t </w>',\n",
       " 'K r a t o s </w>',\n",
       " 'a </w>',\n",
       " 'S p a r t a n </w>',\n",
       " 'w a r r i o r </w>',\n",
       " 'a n d </w>',\n",
       " 'l a t e r </w>',\n",
       " 't h e </w>',\n",
       " 'G o d </w>',\n",
       " 'o f </w>',\n",
       " 'W a r </w>',\n",
       " 'w h o </w>',\n",
       " 'w a s </w>',\n",
       " 't r i c k e d </w>',\n",
       " 'i n t o </w>',\n",
       " 'k i l l i n g </w>',\n",
       " 'h i s </w>',\n",
       " 'f a m i l y </w>',\n",
       " 'b y </w>',\n",
       " 'h i s </w>',\n",
       " 'f o r m e r </w>',\n",
       " 'm a s t e r </w>',\n",
       " 't h e </w>',\n",
       " 'o r i g i n a l </w>',\n",
       " 'G r e e k </w>',\n",
       " 'g o d </w>',\n",
       " 'o f </w>',\n",
       " 'w a r </w>',\n",
       " 'A r e s . </w>',\n",
       " 'T h i s </w>',\n",
       " 's e t s </w>',\n",
       " 'o f f </w>',\n",
       " 'a </w>',\n",
       " 's e r i e s </w>',\n",
       " 'o f </w>',\n",
       " 'e v e n t s </w>',\n",
       " 't h a t </w>',\n",
       " 'l e a d s </w>',\n",
       " 't o </w>',\n",
       " 'w a r s </w>',\n",
       " 'w i t h </w>',\n",
       " 't h e </w>',\n",
       " 'm y t h o l o g i c a l </w>',\n",
       " 'p a n t h e o n s . </w>',\n",
       " 'T h e </w>',\n",
       " 'G r e e k </w>',\n",
       " 'm y t h o l o g y </w>',\n",
       " 'e r a </w>',\n",
       " 'o f </w>',\n",
       " 't h e </w>',\n",
       " 's e r i e s </w>',\n",
       " 's e e s </w>',\n",
       " 'K r a t o s </w>',\n",
       " 'f o l l o w </w>',\n",
       " 'a </w>',\n",
       " 'p a t h </w>',\n",
       " 'o f </w>',\n",
       " 'v e n g e a n c e </w>',\n",
       " 'd u e </w>',\n",
       " 't o </w>',\n",
       " 't h e </w>',\n",
       " 'm a c h i n a t i o n s </w>',\n",
       " 'o f </w>',\n",
       " 't h e </w>',\n",
       " 'O l y m p i a n </w>',\n",
       " 'g o d s </w>',\n",
       " 'w h i l e </w>',\n",
       " 't h e </w>',\n",
       " 'N o r s e </w>',\n",
       " 'm y t h o l o g y </w>',\n",
       " 'e r a </w>',\n",
       " 'w h i c h </w>',\n",
       " 'i n t r o d u c e s </w>',\n",
       " 'h i s </w>',\n",
       " 's o n </w>',\n",
       " 'A t r e u s </w>',\n",
       " 'a s </w>',\n",
       " 'a </w>',\n",
       " 's e c o n d a r y </w>',\n",
       " 'p r o t a g o n i s t </w>',\n",
       " 's h o w s </w>',\n",
       " 'a n </w>',\n",
       " 'o l d e r </w>',\n",
       " 'K r a t o s </w>',\n",
       " 'o n </w>',\n",
       " 'a </w>',\n",
       " 'p a t h </w>',\n",
       " 'o f </w>',\n",
       " 'r e d e m p t i o n </w>',\n",
       " 'w h i c h </w>',\n",
       " 'i n a d v e r t e n t l y </w>',\n",
       " 'b r i n g s </w>',\n",
       " 't h e </w>',\n",
       " 't w o </w>',\n",
       " 'i n t o </w>',\n",
       " 'c o n f l i c t </w>',\n",
       " 'w i t h </w>',\n",
       " 't h e </w>',\n",
       " 'N o r s e </w>',\n",
       " 'g o d s . </w>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initlialize the vocabulary\n",
    "vocab = list(set(\" \".join(corpus)))\n",
    "vocab.remove(' ')\n",
    "\n",
    "#split the word into characters\n",
    "corpus = [\" \".join(token) for token in corpus]\n",
    "\n",
    "#appending </w>\n",
    "corpus=[token+' </w>' for token in corpus]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: {'G o d </w>': 2, 'o f </w>': 9, 'W a r </w>': 2, 'i s </w>': 1, 'a n </w>': 2, 'a c t i o n - a d v e n t u r e </w>': 1, 'g a m e </w>': 2, 'f r a n c h i s e </w>': 1, 'c r e a t e d </w>': 1, 'b y </w>': 2, 'D a v i d </w>': 1, 'J a f f e </w>': 1, 'a t </w>': 1, \"S o n y ' s </w>\": 1, 'S a n t a </w>': 1, 'M o n i c a </w>': 1, 'S t u d i o . </w>': 1, 'I t </w>': 1, 'b e g a n </w>': 1, 'i n </w>': 3, '2 0 0 5 </w>': 1, 'o n </w>': 2, 't h e </w>': 13, 'P l a y S t a t i o n </w>': 2, '2 </w>': 1, '( P S 2 ) </w>': 1, 'v i d e o </w>': 1, 'c o n s o l e </w>': 1, 'a n d </w>': 2, 'h a s </w>': 1, 'b e c o m e </w>': 1, 'a </w>': 7, 'f l a g s h i p </w>': 1, 't i t l e </w>': 1, 'f o r </w>': 1, 'b r a n d </w>': 1, 'c o n s i s t i n g </w>': 1, 'e i g h t </w>': 1, 'g a m e s </w>': 1, 'a c r o s s </w>': 1, 'm u l t i p l e </w>': 1, 'p l a t f o r m s </w>': 1, 'w i t h </w>': 3, 'n i n t h </w>': 1, 'c u r r e n t l y </w>': 1, 'd e v e l o p m e n t . </w>': 1, 'B a s e d </w>': 1, 'a n c i e n t </w>': 1, 'm y t h o l o g y </w>': 3, 's t o r y </w>': 1, 'f o l l o w s </w>': 1, 't i t u l a r </w>': 1, 'p r o t a g o n i s t </w>': 2, 'K r a t o s </w>': 3, 'S p a r t a n </w>': 1, 'w a r r i o r </w>': 1, 'l a t e r </w>': 1, 'w h o </w>': 1, 'w a s </w>': 1, 't r i c k e d </w>': 1, 'i n t o </w>': 2, 'k i l l i n g </w>': 1, 'h i s </w>': 3, 'f a m i l y </w>': 1, 'f o r m e r </w>': 1, 'm a s t e r </w>': 1, 'o r i g i n a l </w>': 1, 'G r e e k </w>': 2, 'g o d </w>': 1, 'w a r </w>': 1, 'A r e s . </w>': 1, 'T h i s </w>': 1, 's e t s </w>': 1, 'o f f </w>': 1, 's e r i e s </w>': 2, 'e v e n t s </w>': 1, 't h a t </w>': 1, 'l e a d s </w>': 1, 't o </w>': 2, 'w a r s </w>': 1, 'm y t h o l o g i c a l </w>': 1, 'p a n t h e o n s . </w>': 1, 'T h e </w>': 1, 'e r a </w>': 2, 's e e s </w>': 1, 'f o l l o w </w>': 1, 'p a t h </w>': 2, 'v e n g e a n c e </w>': 1, 'd u e </w>': 1, 'm a c h i n a t i o n s </w>': 1, 'O l y m p i a n </w>': 1, 'g o d s </w>': 1, 'w h i l e </w>': 1, 'N o r s e </w>': 2, 'w h i c h </w>': 2, 'i n t r o d u c e s </w>': 1, 's o n </w>': 1, 'A t r e u s </w>': 1, 'a s </w>': 1, 's e c o n d a r y </w>': 1, 's h o w s </w>': 1, 'o l d e r </w>': 1, 'r e d e m p t i o n </w>': 1, 'i n a d v e r t e n t l y </w>': 1, 'b r i n g s </w>': 1, 't w o </w>': 1, 'c o n f l i c t </w>': 1, 'g o d s . </w>': 1}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "#returns frequency of each word\n",
    "corpus = collections.Counter(corpus)\n",
    "\n",
    "#convert counter object to dictionary\n",
    "corpus = dict(corpus)\n",
    "print(\"Corpus:\",corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computer frequency of a pair of characters or character sequences\n",
    "#accepts corpus and return frequency of each pair\n",
    "def get_stats(corpus):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in corpus.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merges the most frequent pair in the corpus\n",
    "#accepts the corpus and best pair\n",
    "#returns the modified corpus \n",
    "import re\n",
    "def merge_vocab(pair, corpus_in):\n",
    "    corpus_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    \n",
    "    for word in corpus_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        corpus_out[w_out] = corpus_in[word]\n",
    "    \n",
    "    return corpus_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {('G', 'o'): 2, ('o', 'd'): 6, ('d', '</w>'): 10, ('o', 'f'): 10, ('f', '</w>'): 10, ('W', 'a'): 2, ('a', 'r'): 8, ('r', '</w>'): 10, ('i', 's'): 9, ('s', '</w>'): 29, ('a', 'n'): 13, ('n', '</w>'): 14, ('a', 'c'): 3, ('c', 't'): 2, ('t', 'i'): 9, ('i', 'o'): 7, ('o', 'n'): 17, ('n', '-'): 1, ('-', 'a'): 1, ('a', 'd'): 3, ('d', 'v'): 2, ('v', 'e'): 5, ('e', 'n'): 7, ('n', 't'): 12, ('t', 'u'): 3, ('u', 'r'): 2, ('r', 'e'): 8, ('e', '</w>'): 28, ('g', 'a'): 4, ('a', 'm'): 4, ('m', 'e'): 6, ('f', 'r'): 1, ('r', 'a'): 7, ('n', 'c'): 3, ('c', 'h'): 4, ('h', 'i'): 10, ('s', 'e'): 9, ('c', 'r'): 2, ('e', 'a'): 3, ('a', 't'): 13, ('t', 'e'): 4, ('e', 'd'): 4, ('b', 'y'): 2, ('y', '</w>'): 10, ('D', 'a'): 1, ('a', 'v'): 1, ('v', 'i'): 2, ('i', 'd'): 2, ('J', 'a'): 1, ('a', 'f'): 1, ('f', 'f'): 2, ('f', 'e'): 1, ('t', '</w>'): 8, ('S', 'o'): 1, ('n', 'y'): 1, ('y', \"'\"): 1, (\"'\", 's'): 1, ('S', 'a'): 1, ('t', 'a'): 6, ('a', '</w>'): 11, ('M', 'o'): 1, ('n', 'i'): 4, ('i', 'c'): 6, ('c', 'a'): 2, ('S', 't'): 3, ('u', 'd'): 1, ('d', 'i'): 1, ('o', '.'): 1, ('.', '</w>'): 5, ('I', 't'): 1, ('b', 'e'): 2, ('e', 'g'): 1, ('i', 'n'): 13, ('2', '0'): 1, ('0', '0'): 1, ('0', '5'): 1, ('5', '</w>'): 1, ('t', 'h'): 25, ('h', 'e'): 15, ('P', 'l'): 2, ('l', 'a'): 6, ('a', 'y'): 2, ('y', 'S'): 2, ('2', '</w>'): 1, ('(', 'P'): 1, ('P', 'S'): 1, ('S', '2'): 1, ('2', ')'): 1, (')', '</w>'): 1, ('d', 'e'): 4, ('e', 'o'): 2, ('o', '</w>'): 7, ('c', 'o'): 5, ('n', 's'): 4, ('s', 'o'): 2, ('o', 'l'): 8, ('l', 'e'): 5, ('n', 'd'): 4, ('h', 'a'): 2, ('a', 's'): 5, ('e', 'c'): 2, ('o', 'm'): 1, ('f', 'l'): 2, ('a', 'g'): 3, ('g', 's'): 2, ('s', 'h'): 2, ('i', 'p'): 2, ('p', '</w>'): 1, ('i', 't'): 5, ('t', 'l'): 3, ('f', 'o'): 5, ('o', 'r'): 8, ('b', 'r'): 2, ('s', 'i'): 1, ('s', 't'): 5, ('n', 'g'): 4, ('g', '</w>'): 2, ('e', 'i'): 1, ('i', 'g'): 2, ('g', 'h'): 1, ('h', 't'): 1, ('e', 's'): 6, ('r', 'o'): 4, ('o', 's'): 4, ('s', 's'): 1, ('m', 'u'): 1, ('u', 'l'): 2, ('l', 't'): 1, ('p', 'l'): 2, ('t', 'f'): 1, ('r', 'm'): 2, ('m', 's'): 1, ('w', 'i'): 3, ('h', '</w>'): 8, ('c', 'u'): 1, ('r', 'r'): 2, ('l', 'y'): 4, ('e', 'v'): 2, ('e', 'l'): 1, ('l', 'o'): 7, ('o', 'p'): 1, ('p', 'm'): 1, ('t', '.'): 1, ('B', 'a'): 1, ('c', 'i'): 1, ('i', 'e'): 3, ('m', 'y'): 4, ('y', 't'): 4, ('h', 'o'): 6, ('o', 'g'): 4, ('g', 'y'): 3, ('t', 'o'): 8, ('r', 'y'): 2, ('l', 'l'): 3, ('o', 'w'): 3, ('w', 's'): 2, ('p', 'r'): 2, ('o', 't'): 2, ('g', 'o'): 5, ('K', 'r'): 3, ('S', 'p'): 1, ('p', 'a'): 4, ('r', 't'): 2, ('w', 'a'): 4, ('r', 'i'): 6, ('e', 'r'): 9, ('w', 'h'): 4, ('t', 'r'): 3, ('c', 'k'): 1, ('k', 'e'): 1, ('k', 'i'): 1, ('i', 'l'): 3, ('l', 'i'): 2, ('f', 'a'): 1, ('m', 'i'): 1, ('m', 'a'): 2, ('g', 'i'): 2, ('n', 'a'): 3, ('a', 'l'): 2, ('l', '</w>'): 2, ('G', 'r'): 2, ('e', 'e'): 3, ('e', 'k'): 2, ('k', '</w>'): 2, ('A', 'r'): 1, ('s', '.'): 3, ('T', 'h'): 2, ('e', 't'): 1, ('t', 's'): 2, ('d', 's'): 3, ('r', 's'): 3, ('w', '</w>'): 1, ('g', 'e'): 1, ('c', 'e'): 2, ('d', 'u'): 2, ('u', 'e'): 1, ('O', 'l'): 1, ('y', 'm'): 1, ('m', 'p'): 2, ('p', 'i'): 1, ('i', 'a'): 1, ('N', 'o'): 2, ('u', 'c'): 1, ('A', 't'): 1, ('e', 'u'): 1, ('u', 's'): 1, ('d', 'a'): 1, ('l', 'd'): 1, ('e', 'm'): 1, ('p', 't'): 1, ('t', 'w'): 1, ('w', 'o'): 1, ('n', 'f'): 1})\n"
     ]
    }
   ],
   "source": [
    "#compute frequency of bigrams in a corpus\n",
    "pairs = get_stats(corpus)\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Frequent pair: ('s', '</w>')\n"
     ]
    }
   ],
   "source": [
    "best = max(pairs, key=pairs.get)\n",
    "print(\"Most Frequent pair:\",best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Merging: {'G o d </w>': 2, 'o f </w>': 9, 'W a r </w>': 2, 'i s</w>': 1, 'a n </w>': 2, 'a c t i o n - a d v e n t u r e </w>': 1, 'g a m e </w>': 2, 'f r a n c h i s e </w>': 1, 'c r e a t e d </w>': 1, 'b y </w>': 2, 'D a v i d </w>': 1, 'J a f f e </w>': 1, 'a t </w>': 1, \"S o n y ' s</w>\": 1, 'S a n t a </w>': 1, 'M o n i c a </w>': 1, 'S t u d i o . </w>': 1, 'I t </w>': 1, 'b e g a n </w>': 1, 'i n </w>': 3, '2 0 0 5 </w>': 1, 'o n </w>': 2, 't h e </w>': 13, 'P l a y S t a t i o n </w>': 2, '2 </w>': 1, '( P S 2 ) </w>': 1, 'v i d e o </w>': 1, 'c o n s o l e </w>': 1, 'a n d </w>': 2, 'h a s</w>': 1, 'b e c o m e </w>': 1, 'a </w>': 7, 'f l a g s h i p </w>': 1, 't i t l e </w>': 1, 'f o r </w>': 1, 'b r a n d </w>': 1, 'c o n s i s t i n g </w>': 1, 'e i g h t </w>': 1, 'g a m e s</w>': 1, 'a c r o s s</w>': 1, 'm u l t i p l e </w>': 1, 'p l a t f o r m s</w>': 1, 'w i t h </w>': 3, 'n i n t h </w>': 1, 'c u r r e n t l y </w>': 1, 'd e v e l o p m e n t . </w>': 1, 'B a s e d </w>': 1, 'a n c i e n t </w>': 1, 'm y t h o l o g y </w>': 3, 's t o r y </w>': 1, 'f o l l o w s</w>': 1, 't i t u l a r </w>': 1, 'p r o t a g o n i s t </w>': 2, 'K r a t o s</w>': 3, 'S p a r t a n </w>': 1, 'w a r r i o r </w>': 1, 'l a t e r </w>': 1, 'w h o </w>': 1, 'w a s</w>': 1, 't r i c k e d </w>': 1, 'i n t o </w>': 2, 'k i l l i n g </w>': 1, 'h i s</w>': 3, 'f a m i l y </w>': 1, 'f o r m e r </w>': 1, 'm a s t e r </w>': 1, 'o r i g i n a l </w>': 1, 'G r e e k </w>': 2, 'g o d </w>': 1, 'w a r </w>': 1, 'A r e s . </w>': 1, 'T h i s</w>': 1, 's e t s</w>': 1, 'o f f </w>': 1, 's e r i e s</w>': 2, 'e v e n t s</w>': 1, 't h a t </w>': 1, 'l e a d s</w>': 1, 't o </w>': 2, 'w a r s</w>': 1, 'm y t h o l o g i c a l </w>': 1, 'p a n t h e o n s . </w>': 1, 'T h e </w>': 1, 'e r a </w>': 2, 's e e s</w>': 1, 'f o l l o w </w>': 1, 'p a t h </w>': 2, 'v e n g e a n c e </w>': 1, 'd u e </w>': 1, 'm a c h i n a t i o n s</w>': 1, 'O l y m p i a n </w>': 1, 'g o d s</w>': 1, 'w h i l e </w>': 1, 'N o r s e </w>': 2, 'w h i c h </w>': 2, 'i n t r o d u c e s</w>': 1, 's o n </w>': 1, 'A t r e u s</w>': 1, 'a s</w>': 1, 's e c o n d a r y </w>': 1, 's h o w s</w>': 1, 'o l d e r </w>': 1, 'r e d e m p t i o n </w>': 1, 'i n a d v e r t e n t l y </w>': 1, 'b r i n g s</w>': 1, 't w o </w>': 1, 'c o n f l i c t </w>': 1, 'g o d s . </w>': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#merge the frequent pair in corpus\n",
    "corpus = merge_vocab(best, corpus)\n",
    "print(\"After Merging:\", corpus)\n",
    "\n",
    "#convert a tuple to a string\n",
    "best = \"\".join(list(best))\n",
    "\n",
    "#append to merge list and vocabulary\n",
    "merges = []\n",
    "merges.append(best)\n",
    "vocab.append(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Merge Operations: ['s</w>', 'e</w>', 'th', 'on', 'an', 'in', 'the</w>', 'at', 'a</w>', 'd</w>', 'of']\n"
     ]
    }
   ],
   "source": [
    "num_merges = 10\n",
    "for i in range(num_merges):\n",
    "    \n",
    "    #compute frequency of bigrams in a corpus\n",
    "    pairs = get_stats(corpus)\n",
    "    \n",
    "    #compute the best pair\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    \n",
    "    #merge the frequent pair in corpus\n",
    "    corpus = merge_vocab(best, corpus)\n",
    "    \n",
    "    #append to merge list and vocabulary\n",
    "    merges.append(best)\n",
    "    vocab.append(best)\n",
    "\n",
    "#convert a tuple to a string\n",
    "merges_in_string = [\"\".join(list(i)) for i in merges]\n",
    "print(\"BPE Merge Operations:\",merges_in_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying BPE to OOV word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t h e s i s </w>': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#applying BPE to OOV\n",
    "oov ='thesis'\n",
    "\n",
    "#tokenize OOV into characters\n",
    "oov = \" \".join(list(oov))\n",
    "\n",
    "#append </w> \n",
    "oov = oov + ' </w>'\n",
    "\n",
    "#create a dictionary\n",
    "oov = { oov : 1}\n",
    "print(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1 th e s i s </w>\n",
      "\n",
      "BPE Completed...\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "while(True):\n",
    "\n",
    "    #compute frequency\n",
    "    pairs = get_stats(oov)\n",
    "\n",
    "    #extract keys\n",
    "    pairs = pairs.keys()\n",
    "    # print(pairs)\n",
    "\n",
    "    # print(merges)\n",
    "    \n",
    "    #find the pairs available in the learned operations\n",
    "    ind=[merges.index(i) for i in pairs if i in merges]\n",
    "\n",
    "    if(len(ind)==0):\n",
    "        print(\"\\nBPE Completed...\")\n",
    "        break\n",
    "    \n",
    "    #choose the most frequent learned operation\n",
    "    best = merges[min(ind)]\n",
    "    \n",
    "    #merge the best pair\n",
    "    oov = merge_vocab(best, oov)\n",
    "    \n",
    "    print(\"Iteration \",i+1, list(oov.keys())[0])\n",
    "    i=i+1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d6bb19db9f6479a27fdbf113b0686348fd251b7bd4b0b86a4868d0d2d84af38"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('NLP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
